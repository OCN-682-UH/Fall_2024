<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Week 8a</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Nyssa Silbiger" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/htmltools-fill/fill.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <link href="libs/wordcloud2/wordcloud.css" rel="stylesheet" />
    <script src="libs/wordcloud2/wordcloud2-all.js"></script>
    <script src="libs/wordcloud2/hover.js"></script>
    <script src="libs/wordcloud2-binding/wordcloud2.js"></script>
    <link href="libs/font-awesome-animation/font-awesome-animation-emi.css" rel="stylesheet" />
    <script src="libs/fontawesome/js/fontawesome-all.min.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Week 8a
]
.subtitle[
## Working with words
]
.author[
### Dr. Nyssa Silbiger
]
.institute[
### UH Data Science Fundamentals Fall 2024
]
.date[
### (updated: 2024-10-08)
]

---




&lt;div style = "position:fixed; visibility: hidden"&gt;
`$$\require{color}\definecolor{yellow}{rgb}{1, 0.8, 0.16078431372549}$$`
`$$\require{color}\definecolor{orange}{rgb}{0.96078431372549, 0.525490196078431, 0.203921568627451}$$`
`$$\require{color}\definecolor{green}{rgb}{0, 0.474509803921569, 0.396078431372549}$$`
&lt;/div&gt;

&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  TeX: {
    Macros: {
      yellow: ["{\\color{yellow}{#1}}", 1],
      orange: ["{\\color{orange}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
&lt;/script&gt;

&lt;style&gt;
.yellow {color: #FFCC29;}
.orange {color: #F58634;}
.green {color: #007965;}
&lt;/style&gt;




---
# Outline of class

Working with words

1. Using {stringr} to clean up strings  (part of tidyverse)
2. Intro to regex (regular expressions)  
3. Using {tidytext} for text mining/analysis  
4. Make a wordcloud  



.pull-left[
&lt;img src ="https://stringr.tidyverse.org/logo.png"/&gt;
]
.pull-right[
&lt;img src="https://files.speakerdeck.com/presentations/a2115495fd9e4a31a70ae8258d45b4c8/slide_18.jpg"/&gt;
]

---
# Set-up your script

New packages to install

``` r
install.packages('tidytext')
install.packages('wordcloud2')
install.packages('janeaustenr')
install.packages('stopwords')
```

Libraries to load

``` r
library(here)
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(janeaustenr)
```
---
# Intro to {stringr}

### What is a string?

--

A string and a character are the same thing. You can tell something is a string by the presence of quotations. For example:



``` r
words&lt;-"This is a string"
words
```

```
## [1] "This is a string"
```


--
You can also have several strings in a vector.  


``` r
words_vector&lt;-c("Apples", "Bananas","Oranges")
words_vector
```

```
## [1] "Apples"  "Bananas" "Oranges"
```

---
# Intro to {stringr}

There are 4 basic families of functions in the {stringr} package:
1.  **Manipulation**: these functions allow you to manipulate individual characters within the strings in character vectors.  
2. **Whitespace tools** to add, remove, and manipulate whitespace.  
3. **Locale sensitive operations** whose operations will vary from locale to locale.  
4. **Pattern matching functions.** These recognize four engines of pattern description. The most common is regular expressions, but there are three other tools.  

.center[
&lt;img src="https://writervsworld.files.wordpress.com/2013/09/words1.jpg", width=75%/&gt;
]

---
# Manipulation

Paste words together.  This can be useful if say you have a two columns of treatments and you want to combine them into one (e.g., high temp, low temp and high pH, low pH).

Examples:


``` r
paste("High temp", "Low pH")
```

```
## [1] "High temp Low pH"
```
--
Add a dash in between the words

``` r
paste("High temp", "Low pH", sep = "-")
```

```
## [1] "High temp-Low pH"
```

--
Remove the space in between the words


``` r
paste0("High temp", "Low pH")
```

```
## [1] "High tempLow pH"
```
---
# Manipulation

Working with vectors  


``` r
shapes &lt;- c("Square", "Circle", "Triangle")
paste("My favorite shape is a", shapes)
```

```
## [1] "My favorite shape is a Square"   "My favorite shape is a Circle"  
## [3] "My favorite shape is a Triangle"
```

--


``` r
two_cities &lt;- c("best", "worst")
paste("It was the", two_cities, "of times.")
```

```
## [1] "It was the best of times."  "It was the worst of times."
```

This is very useful when making labels for your plots
---
# Manipulation: individual characters

Let's say you want to know how long a string is:


``` r
shapes # vector of shapes
```

```
## [1] "Square"   "Circle"   "Triangle"
```

``` r
str_length(shapes) # how many letters are in each word?
```

```
## [1] 6 6 8
```
--

Let's say you want to extract specific characters.  Do you work with sequence data? This could be super useful to exact specific bases in a sequence.


``` r
seq_data&lt;-c("ATCCCGTC")

str_sub(seq_data, start = 2, end = 4) # extract the 2nd to 4th AA
```

```
## [1] "TCC"
```

---
# Manipulation

You can also modify strings


``` r
str_sub(seq_data, start = 3, end = 3) &lt;- "A" # add an A in the 3rd position
seq_data
```

```
## [1] "ATACCGTC"
```

--

You can also duplicate patterns in your strings. Here I am duplicating it 2 and 3 times


``` r
str_dup(seq_data, times = c(2, 3)) # times is the number of times to duplicate each string
```

```
## [1] "ATACCGTCATACCGTC"         "ATACCGTCATACCGTCATACCGTC"
```


---
# Whitespace

Say you have a column and you did not copy and paste your treatments like you learned in the first week of class.  You now have some words with extra white spaces and R thinks its an entirely new word.  Here is how to deal with that...


``` r
badtreatments&lt;-c("High", " High", "High ", "Low", "Low")
badtreatments
```

```
## [1] "High"  " High" "High " "Low"   "Low"
```

--

Remove white space


``` r
str_trim(badtreatments) # this removes both 
```

```
## [1] "High" "High" "High" "Low"  "Low"
```

--

You can also just remove from one side or the other  

``` r
str_trim(badtreatments, side = "left") # this removes left 
```

```
## [1] "High"  "High"  "High " "Low"   "Low"
```

---
# Whitespace

The opposite of str_trim is str_pad, to add white space to either side


``` r
str_pad(badtreatments, 5, side = "right") # add a white space to the right side after the 5th character
```

```
## [1] "High " " High" "High " "Low  " "Low  "
```

--
add a character instead of white space

``` r
str_pad(badtreatments, 5, side = "right", pad = "1") # add a 1 to the right side after the 5th character
```

```
## [1] "High1" " High" "High " "Low11" "Low11"
```


---
# Locale sensitive

Important, these will perform differently in different places in the world/with different languages. The default is English, but you can set the language setting.

Make everything upper case


``` r
x&lt;-"I love R!"

str_to_upper(x)
```

```
## [1] "I LOVE R!"
```

--

Make it lower case


``` r
str_to_lower(x)
```

```
## [1] "i love r!"
```

--
Make it title case (Cap first letter of each word)

``` r
str_to_title(x)
```

```
## [1] "I Love R!"
```

---
# Pattern matching

{stringr} has functions to view, detect, locate, extract, match, replace, and split strings based on specific patterns. 

View a specific pattern in a vector of strings.


``` r
data&lt;-c("AAA", "TATA", "CTAG", "GCTT")
```


``` r
# find all the strings with an A
str_view(data, pattern = "A")
```

![](libs/images/viewA.PNG)

---
# Pattern matching

{stringr} has functions to view, detect, locate, extract, match, replace, and split strings based on specific patterns. 

Detect a specific pattern


``` r
str_detect(data, pattern = "A")
```

```
## [1]  TRUE  TRUE  TRUE FALSE
```


``` r
str_detect(data, pattern = "AT")
```

```
## [1] FALSE  TRUE FALSE FALSE
```

--
Locate a pattern

``` r
str_locate(data, pattern = "AT")
```

```
##      start end
## [1,]    NA  NA
## [2,]     2   3
## [3,]    NA  NA
## [4,]    NA  NA
```
---
# regex: regular expressions

But, what if we want to search for something more complicated than that... like find all the numbers, letters, or special characters.

[First, a cheat sheet.](http://edrub.in/CheatSheets/cheatSheetStringr.pdf)

--

**Regular expressions** are hard and a pain in the butt... but, are very helpful to learn.

Actual image of my trying to use regex...  
.center[
&lt;img src="https://media3.giphy.com/media/kf8bMrmElVACLbFCDg/giphy.gif", width =35%/&gt;
]

---
# regex: regular expressions

There are several types of regular expressions:
  - Metacharacters
  - Sequences
  - Quantifiers
  - Character classes
  - POSIX character classes (Portable Operating System Interface)

--

### Metacharacters

Metacharacters: The simplest form of regular expressions are those that match a single character. Most characters, including all letters and digits, are regular expressions that match themselves. For a language like R, there are some special characters that have reserved meaning and they are referred to as ‘Metacharacters”. The metacharacters in Extended Regular Expressions (EREs) are:

. \ | ( ) [ { $ * + ?

---
# Metacharacters

&lt;img src ="https://d2o2utebsixu4k.cloudfront.net/tutorials/topics/images/1568271038916-image6.jpg"/&gt;

Let's say that you have the following set of strings...


``` r
vals&lt;-c("a.b", "b.c","c.d")
```

--
And you want to replace all the "." with a space. Here is how you would do it:


``` r
#string, pattern, replace
str_replace(vals, "\\.", " ")
```

```
## [1] "a b" "b c" "c d"
```

---
# Aside about the functions

Each function in {stringr} has two forms a basic form that searches for the first instance of a character and a *_all that searches for all instances. For example:

--

Let's say we had multiple "." in our character vector

``` r
vals&lt;-c("a.b.c", "b.c.d","c.d.e")
#string, pattern, replace
str_replace(vals, "\\.", " ")
```

```
## [1] "a b.c" "b c.d" "c d.e"
```

str_replace only replaces the **first instance**.  Let's try str_replace_all()

--


``` r
#string, pattern, replace
str_replace_all(vals, "\\.", " ")
```

```
## [1] "a b c" "b c d" "c d e"
```

---
# Sequences

**Sequences**, as the name suggests refers to the sequences of characters which can match. We have shorthand versions (or anchors) for commonly used sequences in R: 

.center[
&lt;img src="https://d2o2utebsixu4k.cloudfront.net/tutorials/topics/images/1568271358029-image12.jpg", width=150%/&gt;
]

--
Let's **subset** the vector to only keep strings with digits


``` r
val2&lt;-c("test 123", "test 456", "test")

str_subset(val2, "\\d")
```

```
## [1] "test 123" "test 456"
```

---
# Character class

A **character class** or character set is a list of characters enclosed by square brackets [ ]. Character sets are used to match only one of the different characters. For example, the regex character class [aA] matches any lower case letter a or any upper case letter A. 

.center[
&lt;img src="https://d2o2utebsixu4k.cloudfront.net/tutorials/topics/images/1568271446941-image8.jpg", width=75%/&gt;
]

--
Let's **count** the number of lowercase vowels in each string


``` r
str_count(val2, "[aeiou]")
```

```
## [1] 1 1 1
```

``` r
# count any digit
str_count(val2, "[0-9]")
```

```
## [1] 3 3 0
```

---
# Quantifiers
| symbol| Meaning|
|---|----|  
|^	|Beginning of String|  
|$	|End of String|
|\n	|Newline|
|+	|One or More of Previous|
|*	|Zero or More of Previous|
|?	|Zero or One of Previous|
|{5}|	Exactly 5 of Previous|
|{2, 5}|	Between 2 and 5 or Previous|
|{2, }	|More than 2 of Previous|

---
# Example: find the phone numbers


``` r
strings&lt;-c("550-153-7578",
         "banana",
         "435.114.7586",
         "home: 672-442-6739")
```

Make a regex that finds all the strings that contain a phone number.  We know there is a specific pattern (3 numbers, 3 numbers, 4 numbers and it can have either a "." or "-" to separate them). Let's also say we know that the first number cannot be a 1

--


``` r
phone &lt;- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"
```

--

``` r
# Which strings contain phone numbers?
str_detect(strings, phone)
```

```
## [1]  TRUE FALSE  TRUE  TRUE
```

---
# Example: find the phone numbers


``` r
# subset only the strings with phone numbers
test&lt;-str_subset(strings, phone)
test
```

```
## [1] "550-153-7578"       "435.114.7586"       "home: 672-442-6739"
```

--
# Think, pair, share

Let's clean it up.  Lets replace all the "." with "-" and extract only the numbers (leaving the letters behind). Remove any extra white space. You can use a sequence of pipes.

---
# Think, pair, share


``` r
test %&gt;%
  str_replace_all(pattern = "\\.", replacement = "-") %&gt;% # replace periods with -
  str_replace_all(pattern = "[a-zA-Z]|\\:", replacement = "") %&gt;% # remove all the things we don't want
  str_trim() # trim the white space
```

```
## [1] "550-153-7578" "435-114-7586" "672-442-6739"
```

---
# tidytext

Package for text mining and making text tidy.  This is very helpful for social sciences or anyone that uses survey data. Also, really helpful for text mining abstracts to write a review paper on a topic.

--

Let's analyze a books by Jane Austen.  

The function to get all of the text from all of Jane Austen's books is austen_books()


``` r
# explore it
head(austen_books())
```

```
## # A tibble: 6 × 2
##   text                    book               
##   &lt;chr&gt;                   &lt;fct&gt;              
## 1 "SENSE AND SENSIBILITY" Sense &amp; Sensibility
## 2 ""                      Sense &amp; Sensibility
## 3 "by Jane Austen"        Sense &amp; Sensibility
## 4 ""                      Sense &amp; Sensibility
## 5 "(1811)"                Sense &amp; Sensibility
## 6 ""                      Sense &amp; Sensibility
```

``` r
tail(austen_books())
```

```
## # A tibble: 6 × 2
##   text                                                               book      
##   &lt;chr&gt;                                                              &lt;fct&gt;     
## 1 "possible, more distinguished in its domestic virtues than in its" Persuasion
## 2 "national importance."                                             Persuasion
## 3 ""                                                                 Persuasion
## 4 ""                                                                 Persuasion
## 5 ""                                                                 Persuasion
## 6 "Finis"                                                            Persuasion
```


---
# tidytext

Let's clean it up and add a column for line and chapter

``` r
original_books &lt;- austen_books() %&gt;% # get all of Jane Austen's books
  group_by(book) %&gt;%
  mutate(line = row_number(), # find every line
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", # count the chapters (starts with the word chapter followed by a digit or roman numeral)
                                                 ignore_case = TRUE)))) %&gt;% #ignore lower or uppercase
  ungroup() # ungroup it so we have a dataframe again

# don't try to view the entire thing... its &gt;73000 lines...
head(original_books)
```

```
## # A tibble: 6 × 4
##   text                    book                 line chapter
##   &lt;chr&gt;                   &lt;fct&gt;               &lt;int&gt;   &lt;int&gt;
## 1 "SENSE AND SENSIBILITY" Sense &amp; Sensibility     1       0
## 2 ""                      Sense &amp; Sensibility     2       0
## 3 "by Jane Austen"        Sense &amp; Sensibility     3       0
## 4 ""                      Sense &amp; Sensibility     4       0
## 5 "(1811)"                Sense &amp; Sensibility     5       0
## 6 ""                      Sense &amp; Sensibility     6       0
```
---
# tidytext

Because we are interest in text mining, we will want to clean this so that there is only one word per row so its **tidy**. In tidytext each word is refered to as a *token*.  The function to unnest the data so that its only one word per row is unnest_tokens().


``` r
tidy_books &lt;- original_books %&gt;%
  unnest_tokens(output = word, input = text) # add a column named word, with the input as the text column

head(tidy_books) # there are now &gt;725,000 rows. Don't view the entire thing!
```

```
## # A tibble: 6 × 4
##   book                 line chapter word       
##   &lt;fct&gt;               &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      
## 1 Sense &amp; Sensibility     1       0 sense      
## 2 Sense &amp; Sensibility     1       0 and        
## 3 Sense &amp; Sensibility     1       0 sensibility
## 4 Sense &amp; Sensibility     3       0 by         
## 5 Sense &amp; Sensibility     3       0 jane       
## 6 Sense &amp; Sensibility     3       0 austen
```

---
# tidytext

OK so we now have &gt;735,000 rows of words.... but, some of these words are kind of useless.  Words that are common and don't really have important meaning (e.g. "and","by","therefore"...).  These are called stopwords.  We can use the function "get_stopwords()" to essentially remove these words from our dataframe. (This function is essentially just a dataframe of unnecessary words)


``` r
#see an example of all the stopwords
head(get_stopwords())
```

```
## # A tibble: 6 × 2
##   word   lexicon 
##   &lt;chr&gt;  &lt;chr&gt;   
## 1 i      snowball
## 2 me     snowball
## 3 my     snowball
## 4 myself snowball
## 5 we     snowball
## 6 our    snowball
```

---
# tidytext

Use what we know from joins to remove all the stopwords



``` r
cleaned_books &lt;- tidy_books %&gt;%
  anti_join(get_stopwords()) # dataframe without the stopwords
```

```
## Joining with `by = join_by(word)`
```

``` r
head(cleaned_books)
```

```
## # A tibble: 6 × 4
##   book                 line chapter word       
##   &lt;fct&gt;               &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      
## 1 Sense &amp; Sensibility     1       0 sense      
## 2 Sense &amp; Sensibility     1       0 sensibility
## 3 Sense &amp; Sensibility     3       0 jane       
## 4 Sense &amp; Sensibility     3       0 austen     
## 5 Sense &amp; Sensibility     5       0 1811       
## 6 Sense &amp; Sensibility    10       1 chapter
```

---
# tidytext

Let's count the most common words across all her books


``` r
cleaned_books %&gt;%
  count(word, sort = TRUE) 
```

```
## # A tibble: 14,375 × 2
##    word      n
##    &lt;chr&gt; &lt;int&gt;
##  1 mr     3015
##  2 mrs    2446
##  3 must   2071
##  4 said   2041
##  5 much   1935
##  6 miss   1855
##  7 one    1831
##  8 well   1523
##  9 every  1456
## 10 think  1440
## # ℹ 14,365 more rows
```

How would we modify this code to count the most popular words by book? What about by each chapter within a book? 
---
# sentiment analysis

There are many ways that we can now analyze this tidy dataset of text. One example is we could do a sentiment analysis (how many positive and negative words) using get_sentiments(). An important note: I was not an English major and I know there are many different lexicons, but I know nothing about them.  Look at the help files if you want to go deeper into this...


``` r
sent_word_counts &lt;- tidy_books %&gt;%
  inner_join(get_sentiments()) %&gt;% # only keep pos or negative words
  count(word, sentiment, sort = TRUE) # count them

head(sent_word_counts)[1:3,]
```

```
## # A tibble: 3 × 3
##   word  sentiment     n
##   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;
## 1 miss  negative   1855
## 2 well  positive   1523
## 3 good  positive   1380
```

--

Now, think about how we could do this with science? Instead of get_sentiments(), you could use an inner_join with a vector of keywords that you are searching for.  

---
# Let's plot it.

We can now use ggplot to visualize counts of positive and negative words in the books


``` r
sent_word_counts %&gt;%
  filter(n &gt; 150) %&gt;% # take only if there are over 150 instances of it
  mutate(n = ifelse(sentiment == "negative", -n, n)) %&gt;% # add a column where if the word is negative make the count negative
  mutate(word = reorder(word, n)) %&gt;% # sort it so it gows from largest to smallest
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```


---
![](15_Strings_files/figure-html/unnamed-chunk-46-1.png)&lt;!-- --&gt;
---
# Make a wordcloud
Use the {wordcloud} package to make an interactive word cloud 


``` r
words&lt;-cleaned_books %&gt;%
  count(word) %&gt;% # count all the words
  arrange(desc(n))%&gt;% # sort the words
  slice(1:100) #take the top 100

wordcloud2(words, shape = 'triangle', size=0.3) # make a wordcloud out of the top 100 words
```

<div class="wordcloud2 html-widget html-fill-item" id="htmlwidget-b85227c012dab7f4063c" style="width:50%;height:504px;"></div>
<script type="application/json" data-for="htmlwidget-b85227c012dab7f4063c">{"x":{"word":["mr","mrs","must","said","much","miss","one","well","every","think","now","good","might","never","time","know","little","can","nothing","though","soon","without","say","great","see","first","may","two","quite","fanny","made","thought","shall","man","dear","ever","lady","always","sir","day","make","room","emma","thing","young","sure","sister","like","however","house","away","long","elizabeth","go","give","indeed","even","last","better","many","upon","way","elinor","felt","enough","come","hope","friend","family","oh","just","done","us","mind","seemed","father","home","jane","wish","mother","still","catherine","feelings","happy","saw","something","moment","came","really","look","place","half","love","till","crawford","marianne","perhaps","yes","yet","almost"],"freq":[3015,2446,2071,2041,1935,1855,1831,1523,1456,1440,1405,1380,1369,1362,1337,1332,1295,1186,1175,1075,1058,1047,1046,981,976,972,956,876,870,862,845,839,834,830,822,821,817,813,806,797,795,790,787,772,766,742,727,725,714,699,693,689,687,684,682,664,644,643,639,635,632,631,623,618,613,608,601,593,578,572,570,569,566,563,563,558,550,550,542,541,540,539,535,534,529,525,515,509,504,503,503,501,495,495,493,492,491,489,488,476],"fontFamily":"Segoe UI","fontWeight":"bold","color":"random-dark","minSize":0,"weightFactor":0.01791044776119403,"backgroundColor":"white","gridSize":0,"minRotation":-0.7853981633974483,"maxRotation":0.7853981633974483,"shuffle":true,"rotateRatio":0.4,"shape":"triangle","ellipticity":0.65,"figBase64":null,"hover":null},"evals":[],"jsHooks":{"render":[{"code":"function(el,x){\n                        console.log(123);\n                        if(!iii){\n                          window.location.reload();\n                          iii = False;\n\n                        }\n  }","data":null}]}}</script>

---
# Today's totally awesome R package

### {ggirl} (gg in real life)

Have you ever made a plot that you love so much and thought, "I want to send this to someone!" Well, now you can! Turn your ggplots into a postcard.  
(*Note, I wouldn't be mad if random postcards of ggplot showed up in my mailbox...*)

[Check out ggirl here](https://jnolis.com/blog/introducing_ggirl/) <span>&lt;i class="fas  fa-trophy faa-tada animated " style=" color:blue;"&gt;&lt;/i&gt;</span>

![](libs/images/ggirl.PNG)
---
# No Homework Today!

Work on your goodplot/badplot

---
class: center, middle

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).  
Many slides modified from the following resources:  
[CRAN stringr](https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html)  
[Mastering software development in R](https://bookdown.org/rdpeng/RProgDA/text-processing-and-regular-expressions.html#regular-expressions)   

[stringr vingette](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html)  

[modern, consistent string processing](https://journal.r-project.org/archive/2010-2/RJournal_2010-2_Wickham.pdf)  

[Intro to tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)  

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false,
"ratio": "15:10"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
